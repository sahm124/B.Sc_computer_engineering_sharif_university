{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5690 images belonging to 24 classes.\n",
      "Found 1208 images belonging to 24 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3s/step - Precision: 0.9623 - Recall: 0.6948 - accuracy: 0.7304 - categorical_accuracy: 0.7304 - loss: 0.9126 - val_Precision: 0.8435 - val_Recall: 0.8121 - val_accuracy: 0.8237 - val_categorical_accuracy: 0.8237 - val_loss: 0.6934\n",
      "Epoch 2/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 3s/step - Precision: 0.9618 - Recall: 0.6869 - accuracy: 0.7324 - categorical_accuracy: 0.7324 - loss: 0.9363 - val_Precision: 0.8415 - val_Recall: 0.8129 - val_accuracy: 0.8228 - val_categorical_accuracy: 0.8228 - val_loss: 0.6946\n",
      "Epoch 3/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 3s/step - Precision: 0.9746 - Recall: 0.7172 - accuracy: 0.7616 - categorical_accuracy: 0.7616 - loss: 0.7990 - val_Precision: 0.8412 - val_Recall: 0.8113 - val_accuracy: 0.8245 - val_categorical_accuracy: 0.8245 - val_loss: 0.6961\n",
      "Epoch 4/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9783 - Recall: 0.7131 - accuracy: 0.7568 - categorical_accuracy: 0.7568 - loss: 0.8241 - val_Precision: 0.8412 - val_Recall: 0.8113 - val_accuracy: 0.8245 - val_categorical_accuracy: 0.8245 - val_loss: 0.6975\n",
      "Epoch 5/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9708 - Recall: 0.7186 - accuracy: 0.7618 - categorical_accuracy: 0.7618 - loss: 0.8141 - val_Precision: 0.8413 - val_Recall: 0.8121 - val_accuracy: 0.8245 - val_categorical_accuracy: 0.8245 - val_loss: 0.6981\n",
      "Epoch 6/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 3s/step - Precision: 0.9834 - Recall: 0.7063 - accuracy: 0.7604 - categorical_accuracy: 0.7604 - loss: 0.8061 - val_Precision: 0.8419 - val_Recall: 0.8113 - val_accuracy: 0.8253 - val_categorical_accuracy: 0.8253 - val_loss: 0.6988\n",
      "Epoch 7/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 3s/step - Precision: 0.9721 - Recall: 0.7031 - accuracy: 0.7492 - categorical_accuracy: 0.7492 - loss: 0.8445 - val_Precision: 0.8413 - val_Recall: 0.8121 - val_accuracy: 0.8270 - val_categorical_accuracy: 0.8270 - val_loss: 0.6991\n",
      "Epoch 8/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 3s/step - Precision: 0.9730 - Recall: 0.6943 - accuracy: 0.7393 - categorical_accuracy: 0.7393 - loss: 0.8578 - val_Precision: 0.8422 - val_Recall: 0.8129 - val_accuracy: 0.8253 - val_categorical_accuracy: 0.8253 - val_loss: 0.7007\n",
      "Epoch 9/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9791 - Recall: 0.7208 - accuracy: 0.7701 - categorical_accuracy: 0.7701 - loss: 0.7670 - val_Precision: 0.8422 - val_Recall: 0.8129 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7010\n",
      "Epoch 10/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9786 - Recall: 0.7123 - accuracy: 0.7636 - categorical_accuracy: 0.7636 - loss: 0.7920 - val_Precision: 0.8425 - val_Recall: 0.8146 - val_accuracy: 0.8270 - val_categorical_accuracy: 0.8270 - val_loss: 0.7013\n",
      "Epoch 11/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9725 - Recall: 0.7086 - accuracy: 0.7509 - categorical_accuracy: 0.7509 - loss: 0.8038 - val_Precision: 0.8420 - val_Recall: 0.8162 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7009\n",
      "Epoch 12/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9768 - Recall: 0.7032 - accuracy: 0.7568 - categorical_accuracy: 0.7568 - loss: 0.8234 - val_Precision: 0.8431 - val_Recall: 0.8137 - val_accuracy: 0.8262 - val_categorical_accuracy: 0.8262 - val_loss: 0.7030\n",
      "Epoch 13/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9763 - Recall: 0.7017 - accuracy: 0.7577 - categorical_accuracy: 0.7577 - loss: 0.8323 - val_Precision: 0.8461 - val_Recall: 0.8146 - val_accuracy: 0.8262 - val_categorical_accuracy: 0.8262 - val_loss: 0.7039\n",
      "Epoch 14/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 3s/step - Precision: 0.9771 - Recall: 0.7172 - accuracy: 0.7627 - categorical_accuracy: 0.7627 - loss: 0.7947 - val_Precision: 0.8464 - val_Recall: 0.8162 - val_accuracy: 0.8253 - val_categorical_accuracy: 0.8253 - val_loss: 0.7036\n",
      "Epoch 15/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9846 - Recall: 0.7127 - accuracy: 0.7693 - categorical_accuracy: 0.7693 - loss: 0.7871 - val_Precision: 0.8464 - val_Recall: 0.8162 - val_accuracy: 0.8245 - val_categorical_accuracy: 0.8245 - val_loss: 0.7036\n",
      "Epoch 16/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 3s/step - Precision: 0.9766 - Recall: 0.7020 - accuracy: 0.7479 - categorical_accuracy: 0.7479 - loss: 0.8401 - val_Precision: 0.8462 - val_Recall: 0.8154 - val_accuracy: 0.8253 - val_categorical_accuracy: 0.8253 - val_loss: 0.7039\n",
      "Epoch 17/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 3s/step - Precision: 0.9784 - Recall: 0.7114 - accuracy: 0.7609 - categorical_accuracy: 0.7609 - loss: 0.7876 - val_Precision: 0.8462 - val_Recall: 0.8154 - val_accuracy: 0.8253 - val_categorical_accuracy: 0.8253 - val_loss: 0.7046\n",
      "Epoch 18/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9764 - Recall: 0.7059 - accuracy: 0.7547 - categorical_accuracy: 0.7547 - loss: 0.8080 - val_Precision: 0.8460 - val_Recall: 0.8137 - val_accuracy: 0.8270 - val_categorical_accuracy: 0.8270 - val_loss: 0.7054\n",
      "Epoch 19/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9824 - Recall: 0.7110 - accuracy: 0.7636 - categorical_accuracy: 0.7636 - loss: 0.8008 - val_Precision: 0.8456 - val_Recall: 0.8162 - val_accuracy: 0.8262 - val_categorical_accuracy: 0.8262 - val_loss: 0.7056\n",
      "Epoch 20/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9742 - Recall: 0.7029 - accuracy: 0.7539 - categorical_accuracy: 0.7539 - loss: 0.8172 - val_Precision: 0.8465 - val_Recall: 0.8171 - val_accuracy: 0.8286 - val_categorical_accuracy: 0.8286 - val_loss: 0.7059\n",
      "Epoch 21/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9764 - Recall: 0.7148 - accuracy: 0.7690 - categorical_accuracy: 0.7690 - loss: 0.7895 - val_Precision: 0.8462 - val_Recall: 0.8154 - val_accuracy: 0.8295 - val_categorical_accuracy: 0.8295 - val_loss: 0.7069\n",
      "Epoch 22/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9788 - Recall: 0.7037 - accuracy: 0.7588 - categorical_accuracy: 0.7588 - loss: 0.8254 - val_Precision: 0.8454 - val_Recall: 0.8146 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7078\n",
      "Epoch 23/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3s/step - Precision: 0.9775 - Recall: 0.7090 - accuracy: 0.7580 - categorical_accuracy: 0.7580 - loss: 0.7832 - val_Precision: 0.8469 - val_Recall: 0.8154 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7080\n",
      "Epoch 24/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3s/step - Precision: 0.9763 - Recall: 0.7060 - accuracy: 0.7594 - categorical_accuracy: 0.7594 - loss: 0.8128 - val_Precision: 0.8472 - val_Recall: 0.8171 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7078\n",
      "Epoch 25/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - Precision: 0.9804 - Recall: 0.6951 - accuracy: 0.7464 - categorical_accuracy: 0.7464 - loss: 0.8538 - val_Precision: 0.8485 - val_Recall: 0.8162 - val_accuracy: 0.8295 - val_categorical_accuracy: 0.8295 - val_loss: 0.7075\n",
      "Epoch 26/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4633s\u001b[0m 53s/step - Precision: 0.9726 - Recall: 0.7083 - accuracy: 0.7615 - categorical_accuracy: 0.7615 - loss: 0.8034 - val_Precision: 0.8471 - val_Recall: 0.8162 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7086\n",
      "Epoch 27/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 3s/step - Precision: 0.9859 - Recall: 0.7176 - accuracy: 0.7641 - categorical_accuracy: 0.7641 - loss: 0.7878 - val_Precision: 0.8471 - val_Recall: 0.8162 - val_accuracy: 0.8278 - val_categorical_accuracy: 0.8278 - val_loss: 0.7085\n",
      "Epoch 28/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 3s/step - Precision: 0.9795 - Recall: 0.7059 - accuracy: 0.7565 - categorical_accuracy: 0.7565 - loss: 0.8253 - val_Precision: 0.8497 - val_Recall: 0.8187 - val_accuracy: 0.8295 - val_categorical_accuracy: 0.8295 - val_loss: 0.7084\n",
      "Epoch 29/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 3s/step - Precision: 0.9779 - Recall: 0.7216 - accuracy: 0.7708 - categorical_accuracy: 0.7708 - loss: 0.7654 - val_Precision: 0.8471 - val_Recall: 0.8162 - val_accuracy: 0.8286 - val_categorical_accuracy: 0.8286 - val_loss: 0.7083\n",
      "Epoch 30/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 3s/step - Precision: 0.9758 - Recall: 0.7057 - accuracy: 0.7532 - categorical_accuracy: 0.7532 - loss: 0.8277 - val_Precision: 0.8488 - val_Recall: 0.8179 - val_accuracy: 0.8286 - val_categorical_accuracy: 0.8286 - val_loss: 0.7087\n",
      "Epoch 31/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 3s/step - Precision: 0.9809 - Recall: 0.7312 - accuracy: 0.7724 - categorical_accuracy: 0.7724 - loss: 0.7604 - val_Precision: 0.8495 - val_Recall: 0.8179 - val_accuracy: 0.8286 - val_categorical_accuracy: 0.8286 - val_loss: 0.7083\n",
      "Epoch 32/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 3s/step - Precision: 0.9763 - Recall: 0.7076 - accuracy: 0.7608 - categorical_accuracy: 0.7608 - loss: 0.8074 - val_Precision: 0.8493 - val_Recall: 0.8162 - val_accuracy: 0.8303 - val_categorical_accuracy: 0.8303 - val_loss: 0.7078\n",
      "Epoch 33/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 3s/step - Precision: 0.9790 - Recall: 0.7028 - accuracy: 0.7527 - categorical_accuracy: 0.7527 - loss: 0.8214 - val_Precision: 0.8479 - val_Recall: 0.8171 - val_accuracy: 0.8303 - val_categorical_accuracy: 0.8303 - val_loss: 0.7078\n",
      "Epoch 34/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 3s/step - Precision: 0.9783 - Recall: 0.6992 - accuracy: 0.7493 - categorical_accuracy: 0.7493 - loss: 0.8397 - val_Precision: 0.8494 - val_Recall: 0.8171 - val_accuracy: 0.8295 - val_categorical_accuracy: 0.8295 - val_loss: 0.7080\n",
      "Epoch 35/50\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 3s/step - Precision: 0.9821 - Recall: 0.7038 - accuracy: 0.7533 - categorical_accuracy: 0.7533 - loss: 0.8246 - val_Precision: 0.8484 - val_Recall: 0.8154 - val_accuracy: 0.8286 - val_categorical_accuracy: 0.8286 - val_loss: 0.7085\n",
      "Epoch 36/50\n",
      "\u001b[1m41/89\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 3s/step - Precision: 0.9712 - Recall: 0.6997 - accuracy: 0.7545 - categorical_accuracy: 0.7545 - loss: 0.8033"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/89\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 3s/step - Precision: 0.9725 - Recall: 0.7007 - accuracy: 0.7559 - categorical_accuracy: 0.7559 - loss: 0.8047"
     ]
    }
   ],
   "source": [
    "### beaware how you load custom model which saved in keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "\n",
    "\n",
    "# parameters\n",
    "model_path = 'O:/project/epochs/mobile_adam_efined50_newmodel_freezed/model_epoch_300.keras'\n",
    "train_dir = 'O:/project/dataset/train'\n",
    "val_dir = 'O:/project/dataset/val'\n",
    "checkpoint_dir = 'O:/project/epochs/mobile_adam_efined50_newmodel_final'\n",
    "base_model_path = 'O:/project/epochs/check_mobilenetv2_adam_4_e50_fined_50/model_epoch_50.keras'\n",
    "npy_path = 'O:/project/epochs/mobile_adam_efined50_newmodel_freezedignored_nodes.npy'\n",
    "ignored_nodes = np.load(npy_path)\n",
    "\n",
    "\n",
    "img_height, img_width = 160, 160\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "initial_learning_rate = 1e-5\n",
    "\n",
    "# functions\n",
    "\n",
    "def albumentations_preprocessing(image):\n",
    "    aug_list = [\n",
    "        A.GaussianBlur(p=0.0),  # original image\n",
    "        A.GaussianBlur(p=0.0),  # original image\n",
    "        A.GaussianBlur(p=1.0),\n",
    "        A.CoarseDropout(max_holes=2, max_height=16, max_width=16, p=1.0),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "        A.ChannelShuffle(p=1.0)\n",
    "    ]\n",
    "\n",
    "    augmentation = random.choice(aug_list)\n",
    "    transform = A.Compose([augmentation])\n",
    "    augmented = transform(image=np.array(image))\n",
    "    return augmented['image']\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history['accuracy']\n",
    "    val_acc = history['val_accuracy']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# main code\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=albumentations_preprocessing\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Define the base model\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(base_model_path)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 3. Remove the last layers (classifier)\n",
    "layer_name = model.layers[-2].name  # Get the name of the layer before the final classifier layer\n",
    "model_without_classifier = models.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "\n",
    "def apply_mask(x, mask):\n",
    "    return x * tf.cast(tf.expand_dims(tf.constant(mask), 0), x.dtype)\n",
    "for layer in model_without_classifier.layers:\n",
    "    layer.trainable = True\n",
    "# Apply the mask using a Lambda layer\n",
    "masked_output = Lambda(lambda x: apply_mask(x, ~ignored_nodes))(model_without_classifier.output)\n",
    "\n",
    "# Define the number of classes for your new Dense layer\n",
    "num_classes = train_generator.num_classes  # Make sure this is defined correctly\n",
    "\n",
    "# Add the Dense layer to the masked output\n",
    "final_output = Dense(num_classes, activation='softmax', name='classifier_dense')(masked_output)\n",
    "\n",
    "# Create the new model\n",
    "model_with_masked_output = models.Model(inputs=model_without_classifier.input, outputs=final_output)\n",
    "\n",
    "\n",
    "\n",
    "model_with_masked_output.load_weights(model_path)\n",
    "\n",
    "\n",
    "\n",
    "model_with_masked_output.compile(optimizer=SGD(learning_rate=initial_learning_rate, momentum=0.0),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'categorical_accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}.keras'),\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch')\n",
    "\n",
    "history = model_with_masked_output.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[checkpoint_callback])\n",
    "\n",
    "with open(f'{checkpoint_dir}/training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "with open(f'{checkpoint_dir}/training_history.pkl', 'rb') as f:\n",
    "    saved_history = pickle.load(f)\n",
    "\n",
    "plot_history(saved_history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
